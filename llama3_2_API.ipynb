{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Log in to Hugging Face (ensure you have your token)\n",
        "login(\"Use your hugginface token\")\n",
        "\n",
        "# Set up device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the Llama 3.2 model\n",
        "llama_32 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "generator = pipeline(model=llama_32, device=device, torch_dtype=torch.bfloat16)\n",
        "\n",
        "# Define a function to generate a response\n",
        "def generate_response(message):\n",
        "    prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant, that solves users queries and Problems. Give short and easy to understand response.\"},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    outputs = generator(\n",
        "        prompt,\n",
        "        do_sample=False,\n",
        "        temperature=1.0,\n",
        "        top_p=1,\n",
        "        max_new_tokens=500\n",
        "    )\n",
        "    x = outputs[0][\"generated_text\"][-1]\n",
        "    if x[\"role\"] == \"assistant\":\n",
        "        return x[\"content\"]\n",
        "    return \"No valid response generated.\"\n",
        "\n",
        "# Set up Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all routes\n",
        "\n",
        "# Define a Flask route for the API\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def generate():\n",
        "    # Get user input from the POST request\n",
        "    data = request.json\n",
        "    message = data.get(\"message\", \"\")\n",
        "\n",
        "    if not message:\n",
        "        return jsonify({\"error\": \"No message provided.\"}), 400\n",
        "\n",
        "    # Generate a response using the Llama 3.2 model\n",
        "    try:\n",
        "        response = generate_response(message)\n",
        "        return jsonify({\"response\": response})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Replace 'Use your ngrok token' with your ngrok authtoken\n",
        "    ngrok.set_auth_token(\"Use your ngrok token\")\n",
        "\n",
        "    # Start ngrok and get the public URL\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "id": "CQU1wgdVB__6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}